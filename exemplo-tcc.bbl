\begin{thebibliography}{\hspace{\parindent}}

\bibitem[Barbero; Banino; Kapturowski; Kumaran; Ara{\'u}jo; Vitvitskyi;
  Pascanu; Veli{\v{c}}kovi{\'c}(2024)Barbero; Banino; Kapturowski; Kumaran;
  Ara{\'u}jo; Vitvitskyi; Pascanu;
  Veli{\v{c}}kovi{\'c}]{barbero2024transformers} BARBERO, F.; BANINO, A.;
  KAPTUROWSKI, S.; KUMARAN, D.; ARA{\'U}JO, J.~G.; VITVITSKYI, A.; PASCANU, R.;
  VELI{\v{C}}KOVI{\'C}, P\@. Transformers need glasses! Information
  over-squashing in language tasks. \textbf{arXiv preprint arXiv:2406.04267},
  [S.l.], 2024.

\bibitem[Beltagy; Peters; Cohan(2020)Beltagy; Peters;
  Cohan]{beltagy2020longformer} BELTAGY, I.; PETERS, M.~E.; COHAN, A\@.
  Longformer: The long-document transformer. In: XIV PREPRINT ARXIV:2004.05150,
  2020. \textbf{Anais{\ldots}} [S.l.:~s.n.], 2020.

\bibitem[Bommasani; Hudson; Adeli; Altman; Arora; Arx; Bernstein; Bohg;
  Bosselut; Brunskill \MakeLowercase{et~al.}(2021)Bommasani; Hudson; Adeli;
  Altman; Arora; Arx; Bernstein; Bohg; Bosselut; Brunskill
  \MakeLowercase{et~al.}]{bommasani2021opportunities} BOMMASANI, R.; HUDSON,
  D.~A.; ADELI, E.; ALTMAN, R.; ARORA, S.; ARX, S.~von; BERNSTEIN, M.~S.; BOHG,
  J.; BOSSELUT, A.; BRUNSKILL, E. \MakeLowercase{et~al.} On the opportunities
  and risks of foundation models. \textbf{arXiv preprint arXiv:2108.07258},
  [S.l.], 2021.

\bibitem[Bonwell; Eison(1991)Bonwell; Eison]{bonwell1991active} BONWELL, C.~C.;
  EISON, J.~A\@. \textbf{Active Learning}: Creating Excitement in the
  Classroom. [S.l.]: ASHE-ERIC Higher Education Report, 1991.

\bibitem[Brown; Mann; Ryder; Subbiah; Kaplan; Dhariwal; Neelakantan; Shyam;
  Sastry; Askell \MakeLowercase{et~al.}(2020)Brown; Mann; Ryder; Subbiah;
  Kaplan; Dhariwal; Neelakantan; Shyam; Sastry; Askell
  \MakeLowercase{et~al.}]{brown2020language} BROWN, T.; MANN, B.; RYDER, N.;
  SUBBIAH, M.; KAPLAN, J.~D.; DHARIWAL, P.; NEELAKANTAN, A.; SHYAM, P.; SASTRY,
  G.; ASKELL, A. \MakeLowercase{et~al.} Language models are few-shot learners.
  \textbf{Advances in neural information processing systems}, [S.l.], v.33,
  p.1877--1901, 2020.

\bibitem[Devlin; Chang; Lee; Toutanova(2019)Devlin; Chang; Lee;
  Toutanova]{devlin2019bert} DEVLIN, J.; CHANG, M.-W.; LEE, K.; TOUTANOVA, K\@.
  BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding. In: CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE
  ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES,
  2019., 2019. \textbf{Proceedings{\ldots}} [S.l.:~s.n.], 2019. p.4171--4186.

\bibitem[Dunlosky; Rawson; Marsh; Nathan; Willingham(2013)Dunlosky; Rawson;
  Marsh; Nathan; Willingham]{dunlosky2013improving} DUNLOSKY, J.; RAWSON,
  K.~A.; MARSH, E.~J.; NATHAN, M.~J.; WILLINGHAM, D.~T\@. Improving students'
  learning with effective learning techniques: Promising directions from
  cognitive and educational psychology. \textbf{Psychological Science in the
  Public Interest}, [S.l.], v.14, n.1, p.4--58, 2013.

\bibitem[Freeman; Eddy; Mcdonough; Smith; Okoroafor; Jordt;
  Wenderoth(2014)Freeman; Eddy; Mcdonough; Smith; Okoroafor; Jordt;
  Wenderoth]{freeman2014active} FREEMAN, S.; EDDY, S.~L.; MCDONOUGH, M.; SMITH,
  M.~K.; OKOROAFOR, N.; JORDT, H.; WENDEROTH, M.~P\@. Active learning increases
  student performance in science, engineering, and mathematics.
  \textbf{Proceedings of the National Academy of Sciences}, [S.l.], v.111,
  n.23, p.8410--8415, 2014.

\bibitem[Frohmann; Sterner; Vulic; Minixhofer; Schedl(2024)Frohmann; Sterner;
  Vulic; Minixhofer; Schedl]{frohmann2024segment} FROHMANN, M.; STERNER, I.;
  VULIC, I.; MINIXHOFER, B.; SCHEDL, M\@. Segment Any Text: A Universal
  Approach for Robust, Efficient and Adaptable Sentence Segmentation.
  \textbf{arXiv preprint arXiv:2406.16678}, [S.l.], 2024.

\bibitem[Gao; Xiong; Gao; Jiang; Shen; Ren; Han(2023)Gao; Xiong; Gao; Jiang;
  Shen; Ren; Han]{gao2023retrieval} GAO, Y.; XIONG, Y.; GAO, X.; JIANG, K.;
  SHEN, J.; REN, X.; HAN, J\@. Retrieval-augmented generation for large
  language models: A survey. \textbf{arXiv preprint arXiv:2312.10997}, [S.l.],
  2023.

\bibitem[Guhr; Schumann; Bahrmann; B\"{o}hme(2021)Guhr; Schumann; Bahrmann;
  B\"{o}hme]{guhr-EtAl:2021:fullstop} GUHR, O.; SCHUMANN, A.-K.; BAHRMANN, F.;
  B\"{o}HME, H.~J\@. FullStop: Multilingual Deep Models for Punctuation
  Prediction. In: SWISS TEXT ANALYTICS CONFERENCE 2021, 2021, Winterthur,
  Switzerland. \textbf{Proceedings{\ldots}} CEUR Workshop Proceedings, 2021.

\bibitem[Guo; Kim; Rubin(2014)Guo; Kim; Rubin]{guo2014video} GUO, P.~J.; KIM,
  J.; RUBIN, R\@. How video production affects student engagement: An empirical
  study of MOOC videos. \textbf{Proceedings of the first ACM conference on
  Learning@ scale conference}, [S.l.], p.41--50, 2014.

\bibitem[Kaplan; Mccandlish; Henighan; Brown; Chess; Child; Gray; Radford; Wu;
  Amodei(2020)Kaplan; Mccandlish; Henighan; Brown; Chess; Child; Gray; Radford;
  Wu; Amodei]{kaplan2020scaling} KAPLAN, J.; MCCANDLISH, S.; HENIGHAN, T.;
  BROWN, T.~B.; CHESS, B.; CHILD, R.; GRAY, S.; RADFORD, A.; WU, J.; AMODEI,
  D\@. Scaling laws for neural language models. \textbf{arXiv preprint
  arXiv:2001.08361}, [S.l.], 2020.

\bibitem[Kojima; Gu; Reid; Matsuo; Iwasawa(2022)Kojima; Gu; Reid; Matsuo;
  Iwasawa]{kojima2022large} KOJIMA, T.; GU, S.~S.; REID, M.; MATSUO, Y.;
  IWASAWA, Y\@. Large language models are zero-shot reasoners. \textbf{Advances
  in Neural Information Processing Systems}, [S.l.], v.35, p.22199--22213,
  2022.

\bibitem[Lewis; Perez; Piktus; Petroni; Karpukhin; Goyal; Küttler; Lewis; Yih;
  Rocktäschel \MakeLowercase{et~al.}(2020)Lewis; Perez; Piktus; Petroni;
  Karpukhin; Goyal; Küttler; Lewis; Yih; Rocktäschel
  \MakeLowercase{et~al.}]{lewis2020retrieval} LEWIS, P.; PEREZ, E.; PIKTUS, A.;
  PETRONI, F.; KARPUKHIN, V.; GOYAL, N.; KüTTLER, H.; LEWIS, M.; YIH, W.-t.;
  ROCKTäSCHEL, T. \MakeLowercase{et~al.} Retrieval-augmented generation for
  knowledge-intensive nlp tasks. \textbf{Advances in Neural Information
  Processing Systems}, [S.l.], v.33, p.9459--9474, 2020.

\bibitem[Liu; Bosselut; Srinivasan; Choi; Hajishirzi; Khashabi(2023)Liu;
  Bosselut; Srinivasan; Choi; Hajishirzi; Khashabi]{liu2023lost} LIU, N.~F.;
  BOSSELUT, A.; SRINIVASAN, D.; CHOI, Y.; HAJISHIRZI, H.; KHASHABI, D\@. Lost
  in the middle: How language models use long contexts. \textbf{arXiv preprint
  arXiv:2307.03172}, [S.l.], 2023.

\bibitem[Liu; Yuan; Fu; Jiang; Hayashi; Neubig(2023)Liu; Yuan; Fu; Jiang;
  Hayashi; Neubig]{liu2023pre} LIU, P.; YUAN, W.; FU, J.; JIANG, Z.; HAYASHI,
  H.; NEUBIG, G\@. Pre-train, prompt, and predict: A systematic survey of
  prompting methods in natural language processing. \textbf{ACM Computing
  Surveys}, [S.l.], v.55, n.9, p.1--35, 2023.

\bibitem[Liu; Sun; Zhong; Han; Sun; Zhang; Peng; Tang; Huang; Lai
  \MakeLowercase{et~al.}(2023)Liu; Sun; Zhong; Han; Sun; Zhang; Peng; Tang;
  Huang; Lai \MakeLowercase{et~al.}]{liu2023evaluating} LIU, Y.; SUN, Y.;
  ZHONG, Y.; HAN, C.; SUN, R.; ZHANG, X.; PENG, Y.; TANG, Z.; HUANG, J.; LAI,
  Z. \MakeLowercase{et~al.} Evaluating large language models: A comprehensive
  survey. \textbf{arXiv preprint arXiv:2310.19736}, [S.l.], 2023.

\bibitem[{OpenAI}(2023a){OpenAI}]{openai2023gpt4} {OpenAI}. GPT-4 Technical
  Report. \textbf{arXiv preprint arXiv:2303.08774}, [S.l.], 2023.

\bibitem[{OpenAI}(2023b){OpenAI}]{openai2023pricing} {OpenAI}.
  \textbf{Pricing}. Accessed: 2023-12-01, \url{https://openai.com/pricing}.

\bibitem[Prince(2004)Prince]{prince2004does} PRINCE, M\@. Does active learning
  work? A review of the research. \textbf{Journal of Engineering Education},
  [S.l.], v.93, n.3, p.223--231, 2004.

\bibitem[Radford; Wu; Child; Luan; Amodei; Sutskever(2019)Radford; Wu; Child;
  Luan; Amodei; Sutskever]{radford2019language} RADFORD, A.; WU, J.; CHILD, R.;
  LUAN, D.; AMODEI, D.; SUTSKEVER, I\@. Language models are unsupervised
  multitask learners. \textbf{OpenAI blog}, [S.l.], v.1, n.8, p.9, 2019.

\bibitem[Raffel; Shazeer; Roberts; Lee; Narang; Matena; Zhou; Li;
  Liu(2020)Raffel; Shazeer; Roberts; Lee; Narang; Matena; Zhou; Li;
  Liu]{raffel2020exploring} RAFFEL, C.; SHAZEER, N.; ROBERTS, A.; LEE, K.;
  NARANG, S.; MATENA, M.; ZHOU, Y.; LI, W.; LIU, P.~J\@. Exploring the limits
  of transfer learning with a unified text-to-text transformer. \textbf{Journal
  of Machine Learning Research}, [S.l.], v.21, p.1--67, 2020.

\bibitem[Schwan; Riempp(2004)Schwan; Riempp]{schwan2004learning} SCHWAN, S.;
  RIEMPP, R\@. Learning by viewing versus learning by doing: Evidence-based
  guidelines for principled learning environments. \textbf{Learning and
  Instruction}, [S.l.], v.14, n.6, p.587--596, 2004.

\bibitem[Singh; Ehtesham; Kumar; Khoei(2025)Singh; Ehtesham; Kumar;
  Khoei]{singh2025agentic} SINGH, A.; EHTESHAM, A.; KUMAR, S.; KHOEI, T.~T\@.
  Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG.
  \textbf{arXiv preprint arXiv:2501.09136}, [S.l.], 2025.

\bibitem[Vaswani; Shazeer; Parmar; Uszkoreit; Jones; Gomez; Kaiser;
  Polosukhin(2017)Vaswani; Shazeer; Parmar; Uszkoreit; Jones; Gomez; Kaiser;
  Polosukhin]{vaswani2017attention} VASWANI, A.; SHAZEER, N.; PARMAR, N.;
  USZKOREIT, J.; JONES, L.; GOMEZ, A.~N.; KAISER, {\L}.; POLOSUKHIN, I\@.
  Attention is all you need. In: ADVANCES IN NEURAL INFORMATION PROCESSING
  SYSTEMS, 2017. \textbf{Anais{\ldots}} [S.l.:~s.n.], 2017. p.5998--6008.

\bibitem[Vemprala; Bonatti; Bucker; Kapoor(2023)Vemprala; Bonatti; Bucker;
  Kapoor]{vemprala2023chatgpt} VEMPRALA, S.; BONATTI, R.; BUCKER, A.; KAPOOR,
  A\@. ChatGPT for robotics: Design principles and model abilities.
  \textbf{arXiv preprint arXiv:2306.17582}, [S.l.], 2023.

\bibitem[Wei; Wang; Schuurmans; Bosma; Ichter; Xia; Chi; Le; Zhou(2022)Wei;
  Wang; Schuurmans; Bosma; Ichter; Xia; Chi; Le; Zhou]{wei2022chain} WEI, J.;
  WANG, X.; SCHUURMANS, D.; BOSMA, M.; ICHTER, B.; XIA, F.; CHI, E.; LE, Q.;
  ZHOU, D\@. Chain of thought prompting elicits reasoning in large language
  models. \textbf{Advances in Neural Information Processing Systems}, [S.l.],
  v.35, p.24824--24837, 2022.

\bibitem[Weng(2023)Weng]{weng2023llm} WENG, L\@. LLM powered autonomous agents.
  \textbf{lilianweng.github.io}, [S.l.], 2023.

\bibitem[White; Fu; Hays; Sandborn; Olea; Gilbert; Elnashar; Spencer-smith;
  Schmidt(2023)White; Fu; Hays; Sandborn; Olea; Gilbert; Elnashar;
  Spencer-smith; Schmidt]{white2023prompt} WHITE, J.; FU, Q.; HAYS, S.;
  SANDBORN, M.; OLEA, C.; GILBERT, H.; ELNASHAR, A.; SPENCER-SMITH, J.;
  SCHMIDT, D.~C\@. Prompt engineering for large language models: A survey.
  \textbf{arXiv preprint arXiv:2307.10169}, [S.l.], 2023.

\bibitem[Xu; Sarthi; Agarwal; Gupta; Saxena; Aralikatte; Batra; Parikh; Misra;
  Awadallah(2023)Xu; Sarthi; Agarwal; Gupta; Saxena; Aralikatte; Batra; Parikh;
  Misra; Awadallah]{xu2023retrieval} XU, Y.; SARTHI, S.; AGARWAL, A.; GUPTA,
  A.; SAXENA, A.; ARALIKATTE, R.; BATRA, D.; PARIKH, D.; MISRA, I.; AWADALLAH,
  A\@. Retrieval-augmented generation for knowledge-intensive nlp tasks.
  \textbf{arXiv preprint arXiv:2305.14002}, [S.l.], 2023.

\bibitem[Yao; Zhao; Yu; Du; Shafran; Narasimhan; Cao(2023)Yao; Zhao; Yu; Du;
  Shafran; Narasimhan; Cao]{yao2023react} YAO, S.; ZHAO, J.; YU, D.; DU, N.;
  SHAFRAN, I.; NARASIMHAN, K.; CAO, Y\@. ReAct: Synergizing reasoning and
  acting in language models. \textbf{arXiv preprint arXiv:2210.03629}, [S.l.],
  2023.

\end{thebibliography}
