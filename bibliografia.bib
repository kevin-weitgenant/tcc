@inproceedings{guhr-EtAl:2021:fullstop,
  title     = {FullStop: Multilingual Deep Models for Punctuation Prediction},
  author    = {Guhr, Oliver and Schumann, Anne-Kathrin and Bahrmann, Frank and B\"{o}hme, Hans Joachim},
  booktitle = {Proceedings of the Swiss Text Analytics Conference 2021},
  month     = jun,
  year      = {2021},
  address   = {Winterthur, Switzerland},
  publisher = {CEUR Workshop Proceedings}
}


@article{barbero2024transformers,
  title   = {Transformers need glasses! Information over-squashing in language tasks},
  author  = {Barbero, Federico and Banino, Andrea and Kapturowski, Steven and Kumaran, Dharshan and Ara{\'u}jo, Jo{\~a}o GM and Vitvitskyi, Alex and Pascanu, Razvan and Veli{\v{c}}kovi{\'c}, Petar},
  journal = {arXiv preprint arXiv:2406.04267},
  year    = {2024}
}

@article{frohmann2024segment,
  title   = {Segment Any Text: A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation},
  author  = {Frohmann, Markus and Sterner, Igor and Vulic, Ivan and Minixhofer, Benjamin and Schedl, Markus},
  journal = {arXiv preprint arXiv:2406.16678},
  year    = {2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4171--4186},
  year={2019}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020}
}

@article{liu2021self,
  title={Self-supervised learning: Generative or contrastive},
  author={Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Mian, Li and Wang, Zhaoyu and Zhang, Jing and Tang, Jie},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={35},
  number={1},
  pages={677--694},
  year={2021}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{sennrich2016neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},
  pages={1715--1725},
  year={2016}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wang2023self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2023}
}

@article{yao2023react,
  title={ReAct: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2023}
}

@article{lu2022fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2022}
}

@article{white2023prompt,
  title={Prompt engineering for large language models: A survey},
  author={White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C},
  journal={arXiv preprint arXiv:2307.10169},
  year={2023}
}

@article{kasneci2023chatgpt,
  title={ChatGPT for good? On opportunities and challenges of large language models for education},
  author={Kasneci, Enkelejda and Sessler, Kathrin and Küchemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and Günnemann, Stephan and Hüllermeier, Eyke and others},
  journal={Learning and Individual Differences},
  volume={103},
  pages={102274},
  year={2023}
}

@article{liu2023lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Bosselut, Antoine and Srinivasan, Dora and Choi, Yejin and Hajishirzi, Hannaneh and Khashabi, Daniel},
  journal={arXiv preprint arXiv:2307.03172},
  year={2023}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jiang, Kangning and Shen, Jiawei and Ren, Xiang and Han, Jiawei},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@inproceedings{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  booktitle={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{xu2023retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Xu, Yuchen and Sarthi, Sanjay and Agarwal, Abhishek and Gupta, Abhirut and Saxena, Abhinav and Aralikatte, Rahul and Batra, Dzmitry and Parikh, Devi and Misra, Ishan and Awadallah, Ahmed},
  journal={arXiv preprint arXiv:2305.14002},
  year={2023}
}

@misc{openai2023pricing,
  title={Pricing},
  author={{OpenAI}},
  howpublished={\url{https://openai.com/pricing}},
  year={2023},
  note={Accessed: 2023-12-01}
}